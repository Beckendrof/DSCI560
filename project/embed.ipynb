{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Abhinav/opt/anaconda3/envs/ds560/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import json\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('piazza.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    processed_posts = []\n",
    "    processed_comments_replies = []\n",
    "\n",
    "    for entry in data:\n",
    "        post_text = entry[\"text\"]\n",
    "        doc = nlp(post_text)\n",
    "        tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "        processed_post = ' '.join(tokens)\n",
    "        processed_posts.append(processed_post)\n",
    "\n",
    "        comments = []\n",
    "        replies = []\n",
    "        for comment_entry in entry[\"comments\"]:\n",
    "            comment_text = comment_entry[\"text\"]\n",
    "            doc = nlp(comment_text)\n",
    "            tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "            processed_comment = ' '.join(tokens)\n",
    "            comments.append(processed_comment)\n",
    "\n",
    "            try:\n",
    "                for reply_entry in comment_entry[\"replies\"]:\n",
    "                    reply_text = reply_entry[\"text\"]\n",
    "                    doc = nlp(reply_text)\n",
    "                    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "                    processed_reply = ' '.join(tokens)\n",
    "                    replies.append(processed_reply)\n",
    "            except:\n",
    "                replies.append([])\n",
    "\n",
    "        processed_comments_replies.append((comments, replies))\n",
    "\n",
    "    return processed_posts, processed_comments_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_embeddings(texts):\n",
    "    if not texts or not texts[0]:  # Check if texts is empty or if the inner list is empty\n",
    "        return None\n",
    "\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts, comments_replies = preprocess_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_embeddings = generate_bert_embeddings(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_embeddings = []\n",
    "comment_data = []\n",
    "count = 0\n",
    "\n",
    "for comments, replies in comments_replies:\n",
    "    comment_emb = generate_bert_embeddings(comments)\n",
    "    for reply in replies:\n",
    "        reply_emb = generate_bert_embeddings(reply)\n",
    "        reply_embeddings.append(reply_emb)\n",
    "    comment_data.append({\"comment_embedding\":comment_emb, \"reply_embeddings\": reply_embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data = []\n",
    "\n",
    "for i, (post, (comments, replies)) in enumerate(zip(posts, comments_replies)):\n",
    "    post_data = {\n",
    "        \"post_text\": post,\n",
    "        \"post_embedding\": post_embeddings[i],\n",
    "        \"comments\": []\n",
    "    }\n",
    "    \n",
    "    for comment, comment_info in zip(comments, comment_data):\n",
    "        comment_emb = comment_info[\"comment_embedding\"]\n",
    "        reply_embeddings = comment_info[\"reply_embeddings\"]\n",
    "        comment_data_info = {\n",
    "            \"comment_text\": comment,\n",
    "            \"comment_embedding\": comment_emb,\n",
    "            \"replies\": []\n",
    "        }\n",
    "        \n",
    "        for reply, reply_emb in zip(replies, reply_embeddings):\n",
    "            reply_data = {\n",
    "                \"reply_text\": reply,\n",
    "                \"reply_embedding\": reply_emb\n",
    "            }\n",
    "            comment_data_info[\"replies\"].append(reply_data)\n",
    "        \n",
    "        post_data[\"comments\"].append(comment_data_info)\n",
    "\n",
    "    matched_data.append(post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been stored in 'embeddings.json'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert NumPy arrays to Python lists\n",
    "def convert_to_json_serializable(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.tolist()\n",
    "    elif isinstance(data, dict):\n",
    "        return {key: convert_to_json_serializable(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_to_json_serializable(item) for item in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Convert NumPy arrays in matched_data to Python lists\n",
    "matched_data_serializable = convert_to_json_serializable(matched_data)\n",
    "\n",
    "# Write to JSON file\n",
    "output_file = \"embeddings.json\"\n",
    "with open(output_file, \"w\") as json_file:\n",
    "    json.dump(matched_data_serializable, json_file, indent=4)\n",
    "\n",
    "print(f\"Embeddings have been stored in '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds560",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
